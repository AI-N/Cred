{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2c12601-78da-44d2-9973-cecb71e120ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: C:\\Users\\User\\Desktop\\MRes\\src\\Cred\n",
      "asasa\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "the :mod:`knns` module includes some k-NN inspired algorithms.\n",
    "\"\"\"\n",
    "import heapq\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from surprise import AlgoBase, Dataset, Reader, accuracy, KNNWithMeans, KNNBasic, KNNBaseline, SVD\n",
    "from surprise.model_selection import cross_validate\n",
    "from surprise import PredictionImpossible\n",
    "from surprise.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import os\n",
    "\n",
    "current_directory = os.getcwd()\n",
    "print(\"Current working directory:\", current_directory)\n",
    "print('asasa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fab1b1-9202-464b-b1ec-46dd09a6ff58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c661324-10cc-4287-89c1-a5bea6ec4614",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def jaccard_sim(list1, list2):\n",
    "    new_list1 = [x for i, x in enumerate(list1) if x != 0 or list2[i] != 0]\n",
    "    new_list2 = [x for i, x in enumerate(list2) if x != 0 or list1[i] != 0]\n",
    "\n",
    "    # Compute the intersection of the two lists, where each 1 or 0 is considered a value\n",
    "    intersection = sum([1 for i in range(len(list1)) if list1[i] == 1 and list2[i] == 1])\n",
    "\n",
    "    # Compute the union of the two lists, where each 1 or 0 is considered a value\n",
    "    union = sum([1 for i in range(len(list1)) if list1[i] == 1 or list2[i] == 1])\n",
    "\n",
    "    # Compute the Jaccard similarity\n",
    "    jaccard_similarity = intersection / union\n",
    "    return jaccard_similarity\n",
    "    \n",
    "# Load movie semantic data\n",
    "u_header = ['item_id', 'title', 'genres']\n",
    "items_df = pd.read_csv('ml-1m/movies.dat', sep='::', names=u_header, encoding='latin', engine='python')\n",
    "items_df = items_df.drop('title', axis = 1)\n",
    "\n",
    "items_dict = items_df.set_index('item_id').T.to_dict('dict')\n",
    "genre_list = ['Action', 'Adventure', 'Animation', 'Children\\'s', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy', 'Film-Noir', 'Horror', 'Musical', 'Mystery','Romance','Sci-Fi','Thriller','War','Western']\n",
    "\n",
    "itemSemanticMatrix = pd.DataFrame(columns=['item_id1', 'item_id2', 'jaccard_sim_value'])\n",
    "\n",
    "for item_id1, item_genres1 in items_dict.items():\n",
    "    genre_flags1 = [int(g in item_genres1['genres']) for g in genre_list]\n",
    "    for item_id2, item_genres2 in items_dict.items():\n",
    "        genre_flags2 = [int(g in item_genres2['genres']) for g in genre_list]\n",
    "        jaccard_list = [item_id1, item_id2, jaccard_sim(genre_flags1, genre_flags2)]\n",
    "        jaccard_df = pd.DataFrame([jaccard_list], columns=['item_id1', 'item_id2', 'jaccard_sim'])\n",
    "        #itemSemanticMatrix.append(jaccard_list, ignore_index=True)\n",
    "        itemSemanticMatrix = pd.concat([itemSemanticMatrix, jaccard_df], ignore_index=True)\n",
    "\n",
    "itemSemanticMatrix = itemSemanticMatrix.set_index(['item_id1', 'item_id2'])\n",
    "\n",
    "print('Calculated Semantic Matrix')\n",
    "\n",
    "print(itemSemanticMatrix.loc[(3, 6), 'jaccard_sim_value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ea504dbc-ad42-41ab-8a74-24f33712ac90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.  0.5 ... 0.5 0.  0.5]\n",
      " [0.  1.  0.  ... 0.  0.  0. ]\n",
      " [0.5 0.  1.  ... 0.5 0.  0.5]\n",
      " ...\n",
      " [0.5 0.  0.5 ... 1.  0.  1. ]\n",
      " [0.  0.  0.  ... 0.  1.  0. ]\n",
      " [0.5 0.  0.5 ... 1.  0.  1. ]]\n",
      "Calculated Demographic Matrix\n"
     ]
    }
   ],
   "source": [
    "def classify_age(age):\n",
    "    if age <= 13: #17\n",
    "        return 1 #'Children'\n",
    "    elif age <= 25: #30\n",
    "        return 2 #'Teenagers'\n",
    "    elif age <= 40: #45\n",
    "        return 3 #'Young Adults'\n",
    "    elif age <= 50: #60\n",
    "        return 4 #'Adults'\n",
    "    else:\n",
    "        return 5 #'Seniors'\n",
    "    \n",
    "# Load movie semantic data\n",
    "u_header = ['user_id', 'age', 'sex','job', 'zipcode']\n",
    "users_df = pd.read_csv('data/u.user', sep='|', names=u_header, encoding='latin')\n",
    "users_df = users_df.drop('zipcode', axis = 1)\n",
    "#users_df = users_df.drop('job', axis = 1)\n",
    "#users_df = users_df.drop('sex', axis = 1)\n",
    "\n",
    "users_df['ageGroup'] = users_df['age'].apply(classify_age)\n",
    "users_df = users_df.drop('age', axis = 1)\n",
    "\n",
    "users_dict = users_df.set_index('user_id').T.to_dict('dict')\n",
    "#print(users_dict)\n",
    "\n",
    "occup_list = [\"administrator\", \"artist\", \"doctor\", \"educator\", \"engineer\", \"entertainment\", \"executive\", \"healthcare\", \"homemaker\", \"lawyer\", \"librarian\", \"marketing\", \"none\", \"other\", \"programmer\", \"retired\", \"salesman\", \"scientist\", \"student\", \"technician\", \"writer\"]\n",
    "age_group = [1, 2, 3, 4, 5]\n",
    "\n",
    "userDemoGraphicMatrix = np.zeros((len(users_df), len(users_df)))\n",
    "\n",
    "for user_id1, user_demo1 in users_dict.items():\n",
    "    \n",
    "    # 5 age group, 1 gender, 21 occupation\n",
    "    demo_values1 = [0 for i in range(27)]    \n",
    "    values1 = list(user_demo1.values())\n",
    "    if(values1[0] == \"M\"):\n",
    "        demo_values1[0] = 1\n",
    "    for oc in range(len(occup_list)):\n",
    "        if(values1[1] == occup_list[oc]):\n",
    "            demo_values1[1+oc] = 1 \n",
    "    for ag in range(len(age_group)):\n",
    "        if(values1[2] == age_group[ag]):\n",
    "            demo_values1[22+ag] = 1\n",
    "\n",
    "    for user_id2, user_demo2 in users_dict.items():\n",
    "        demo_values2 = [0 for i in range(27)]\n",
    "        values2 = list(user_demo2.values())\n",
    "        #print(user_id1, values1[0], values1[1], values1[2])\n",
    "        if(values2[0] == \"M\"):\n",
    "            demo_values2[0] = 1\n",
    "        for oc2 in range(len(occup_list)):\n",
    "            if(values2[1] == occup_list[oc2]):\n",
    "                demo_values2[1+oc2] = 1\n",
    "        for ag2 in range(len(age_group)):\n",
    "            if(values2[2] == age_group[ag2]):\n",
    "                demo_values2[22+ag2] = 1\n",
    "        userDemoGraphicMatrix[user_id1-1, user_id2-1] = jaccard_sim(demo_values1, demo_values2)\n",
    "        \n",
    "print(userDemoGraphicMatrix)\n",
    "print('Calculated Demographic Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2201391f-3f86-416d-9598-222f2bb6dddd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5f1fc0ef-afab-438d-8884-236c16f13538",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Important note: as soon as an algorithm uses a similarity measure, it should\n",
    "# also allow the bsl_options parameter because of the pearson_baseline\n",
    "# similarity. It can be done explicitly (e.g. KNNBaseline), or implicetely\n",
    "# using kwargs (e.g. KNNBasic).\n",
    "\n",
    "class SymmetricAlgo2(AlgoBase):\n",
    "    \"\"\"This is an abstract class aimed to ease the use of symmetric algorithms.\n",
    "    A symmetric algorithm is an algorithm that can can be based on users or on\n",
    "    items indifferently, e.g. all the algorithms in this module.\n",
    "    When the algo is user-based x denotes a user and y an item. Else, it's\n",
    "    reversed.\n",
    "    \"\"\"\n",
    "    def __init__(self, sim_options={}, verbose=True, **kwargs):\n",
    "\n",
    "        AlgoBase.__init__(self, sim_options=sim_options, **kwargs)\n",
    "        self.verbose = verbose\n",
    "        self.isSemantic = sim_options[\"isSemantic\"]\n",
    "        self.isDemographic = sim_options[\"isDemographic\"]\n",
    "        self.isCr = sim_options[\"isCr\"]\n",
    "        self.printLimit = 10\n",
    "        if(self.isCr):\n",
    "            %store -r Cr\n",
    "            self.Cr = Cr\n",
    "            print(Cr[0], Cr[1])\n",
    "        \n",
    "        if(self.isDemographic):\n",
    "           print('hihiihihi')\n",
    "\n",
    "    def fit(self, trainset):\n",
    "\n",
    "        AlgoBase.fit(self, trainset)\n",
    "\n",
    "        ub = self.sim_options[\"user_based\"]\n",
    "        self.n_x = self.trainset.n_users if ub else self.trainset.n_items\n",
    "        self.n_y = self.trainset.n_items if ub else self.trainset.n_users\n",
    "        self.xr = self.trainset.ur if ub else self.trainset.ir\n",
    "        self.yr = self.trainset.ir if ub else self.trainset.ur\n",
    "                \n",
    "        # Initilizating Semantic matrix\n",
    "        if self.isSemantic:\n",
    "            self.SemanticMatrix = np.zeros((self.n_x, self.n_x))\n",
    "            for i in self.trainset.all_items():\n",
    "                for j in self.trainset.all_items():\n",
    "                    self.SemanticMatrix[i, j] = itemSemanticMatrix[int(self.trainset.to_raw_iid(i))-1, int(self.trainset.to_raw_iid(j))-1]\n",
    "                    \n",
    "        # Initilizating Demographic matrix\n",
    "        if self.isDemographic:\n",
    "            self.DemographicMatrix = np.zeros((self.n_x, self.n_x))\n",
    "            for i in self.trainset.all_users():\n",
    "                for j in self.trainset.all_users():\n",
    "                    self.DemographicMatrix[i, j] = userDemoGraphicMatrix[int(self.trainset.to_raw_uid(i))-1, int(self.trainset.to_raw_uid(j))-1]\n",
    "                    \n",
    "        # Initilizating Cr matrix\n",
    "        if self.isCr:\n",
    "            self.CrMatrix = np.zeros((self.n_x, self.n_x))\n",
    "            for i in self.trainset.all_users():\n",
    "                #if(self.printLimit<20):\n",
    "                #        print(self.Cr[int(self.trainset.to_raw_uid(i))-1], 'id:', int(self.trainset.to_raw_uid(i)))\n",
    "                #        self.printLimit += 1\n",
    "                for j in self.trainset.all_users():\n",
    "                    self.CrMatrix[i, j] = self.Cr[int(self.trainset.to_raw_uid(i))-1]\n",
    "                    if(i == j):\n",
    "                        self.CrMatrix[i, j] = 1\n",
    "        \n",
    "        return self\n",
    "\n",
    "    def switch(self, u_stuff, i_stuff):\n",
    "        \"\"\"Return x_stuff and y_stuff depending on the user_based field.\"\"\"\n",
    "\n",
    "        if self.sim_options[\"user_based\"]:\n",
    "            return u_stuff, i_stuff\n",
    "        else:\n",
    "            return i_stuff, u_stuff\n",
    "\n",
    "class KNNBasic2(SymmetricAlgo2):\n",
    "    \"\"\"A basic collaborative filtering algorithm.\n",
    "    Args:\n",
    "        k(int): The (max) number of neighbors to take into account for\n",
    "            aggregation (see :ref:`this note <actual_k_note>`). Default is\n",
    "            ``40``.\n",
    "        min_k(int): The minimum number of neighbors to take into account for\n",
    "            aggregation. If there are not enough neighbors, the prediction is\n",
    "            set to the global mean of all ratings. Default is ``1``.\n",
    "        sim_options(dict): A dictionary of options for the similarity\n",
    "            measure. See :ref:`similarity_measures_configuration` for accepted\n",
    "            options.\n",
    "        verbose(bool): Whether to print trace messages of bias estimation,\n",
    "            similarity, etc.  Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=40, min_k=1, sim_options={}, verbose=True, **kwargs):\n",
    "\n",
    "        SymmetricAlgo2.__init__(self, sim_options=sim_options, verbose=verbose, **kwargs)\n",
    "        self.k = k\n",
    "        self.min_k = min_k\n",
    "        self.count = 0        \n",
    "\n",
    "    def fit(self, trainset):\n",
    "\n",
    "        SymmetricAlgo2.fit(self, trainset)\n",
    "\n",
    "        if(self.isSemantic):\n",
    "            self.sim = self.compute_similarities() *self.SemanticMatrix\n",
    "            #self.sim = self.SemanticMatrix\n",
    "        elif(self.isDemographic):\n",
    "            self.sim = self.compute_similarities()*self.DemographicMatrix\n",
    "        else:\n",
    "            self.sim = self.compute_similarities()\n",
    "                 \n",
    "        #print(self.sim)\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible(\"User and/or item is unknown.\")\n",
    "\n",
    "        x, y = self.switch(u, i)\n",
    "\n",
    "        neighbors = [(self.sim[x, x2], r) for (x2, r) in self.yr[y]]\n",
    "        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[0])\n",
    "        \n",
    "        #print(neighbors)\n",
    "        \n",
    "        # compute weighted average\n",
    "        sum_sim = sum_ratings = actual_k = 0\n",
    "        for (sim, r) in k_neighbors:\n",
    "            if sim > 0:\n",
    "                sum_sim += sim\n",
    "                sum_ratings += sim * r\n",
    "                actual_k += 1\n",
    "\n",
    "        if actual_k < self.min_k:\n",
    "            raise PredictionImpossible(\"Not enough neighbors.\")\n",
    "\n",
    "        est = sum_ratings / sum_sim\n",
    "        \n",
    "        self.count += 1\n",
    "        #print('Count-', self.count, ':x = ',x,'y = ', y)\n",
    "\n",
    "        details = {\"actual_k\": actual_k}\n",
    "        return est, details\n",
    "\n",
    "#================================ KNNBaseline ==============================#\n",
    "\n",
    "class KNNBaseline2(SymmetricAlgo2):\n",
    "    \"\"\"A basic collaborative filtering algorithm taking into account a *baseline* rating.\n",
    "    Args:\n",
    "        k(int): The (max) number of neighbors to take into account for\n",
    "            aggregation (see :ref:`this note <actual_k_note>`). Default is\n",
    "            ``40``.\n",
    "        min_k(int): The minimum number of neighbors to take into account for\n",
    "            aggregation. If there are not enough neighbors, the neighbor\n",
    "            aggregation is set to zero (so the prediction ends up being\n",
    "            equivalent to the baseline). Default is ``1``.\n",
    "        sim_options(dict): A dictionary of options for the similarity\n",
    "            measure. See :ref:`similarity_measures_configuration` for accepted\n",
    "            options. It is recommended to use the :func:`pearson_baseline\n",
    "            <surprise.similarities.pearson_baseline>` similarity measure.\n",
    "        bsl_options(dict): A dictionary of options for the baseline estimates\n",
    "            computation. See :ref:`baseline_estimates_configuration` for\n",
    "            accepted options.\n",
    "        verbose(bool): Whether to print trace messages of bias estimation,\n",
    "            similarity, etc.  Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, k=40, min_k=1, sim_options={}, bsl_options={}, verbose=True, **kwargs\n",
    "    ):\n",
    "\n",
    "        SymmetricAlgo2.__init__(\n",
    "            self,\n",
    "            sim_options=sim_options,\n",
    "            bsl_options=bsl_options,\n",
    "            verbose=verbose,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "        self.k = k\n",
    "        self.min_k = min_k\n",
    "\n",
    "    def fit(self, trainset):\n",
    "\n",
    "        SymmetricAlgo2.fit(self, trainset)\n",
    "\n",
    "        self.bu, self.bi = self.compute_baselines()\n",
    "        self.bx, self.by = self.switch(self.bu, self.bi)\n",
    "        \n",
    "        if(self.isSemantic):\n",
    "            self.sim = self.compute_similarities()*self.SemanticMatrix\n",
    "        elif(self.isDemographic):\n",
    "            #%store -r Cr\n",
    "            self.sim = self.compute_similarities()*self.DemographicMatrix\n",
    "            #print(self.sim.shape())\n",
    "        else:\n",
    "            self.sim = self.compute_similarities()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "\n",
    "        est = self.trainset.global_mean\n",
    "        if self.trainset.knows_user(u):\n",
    "            est += self.bu[u]\n",
    "        if self.trainset.knows_item(i):\n",
    "            est += self.bi[i]\n",
    "\n",
    "        x, y = self.switch(u, i)\n",
    "\n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            return est\n",
    "\n",
    "        neighbors = [(x2, self.sim[x, x2], r) for (x2, r) in self.yr[y]]\n",
    "        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[1])\n",
    "\n",
    "        # compute weighted average\n",
    "        sum_sim = sum_ratings = actual_k = 0\n",
    "        for (nb, sim, r) in k_neighbors:\n",
    "            if sim > 0:\n",
    "                sum_sim += sim\n",
    "                nb_bsl = self.trainset.global_mean + self.bx[nb] + self.by[y]\n",
    "                sum_ratings += sim * (r - nb_bsl)\n",
    "                actual_k += 1\n",
    "\n",
    "        if actual_k < self.min_k:\n",
    "            sum_ratings = 0\n",
    "\n",
    "        try:\n",
    "            est += sum_ratings / sum_sim\n",
    "        except ZeroDivisionError:\n",
    "            pass  # just baseline again\n",
    "\n",
    "        details = {\"actual_k\": actual_k}\n",
    "        return est, details\n",
    "\n",
    "    \n",
    "#================================ KNNWithMeans2 ==============================#\n",
    "\n",
    "class KNNWithMeans2(SymmetricAlgo2):\n",
    "    \"\"\"A basic collaborative filtering algorithm, taking into account the mean\n",
    "    ratings of each user.\n",
    "    Args:\n",
    "        k(int): The (max) number of neighbors to take into account for\n",
    "            aggregation (see :ref:`this note <actual_k_note>`). Default is\n",
    "            ``40``.\n",
    "        min_k(int): The minimum number of neighbors to take into account for\n",
    "            aggregation. If there are not enough neighbors, the neighbor\n",
    "            aggregation is set to zero (so the prediction ends up being\n",
    "            equivalent to the mean :math:`\\\\mu_u` or :math:`\\\\mu_i`). Default is\n",
    "            ``1``.\n",
    "        sim_options(dict): A dictionary of options for the similarity\n",
    "            measure. See :ref:`similarity_measures_configuration` for accepted\n",
    "            options.\n",
    "        verbose(bool): Whether to print trace messages of bias estimation,\n",
    "            similarity, etc.  Default is True.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, k=40, min_k=1, sim_options={}, verbose=True, **kwargs):\n",
    "\n",
    "        SymmetricAlgo2.__init__(self, sim_options=sim_options, verbose=verbose, **kwargs)\n",
    "\n",
    "        self.k = k\n",
    "        self.min_k = min_k\n",
    "\n",
    "    def fit(self, trainset):\n",
    "\n",
    "        SymmetricAlgo2.fit(self, trainset)\n",
    "        \n",
    "        self.sim = self.compute_similarities()\n",
    "        \n",
    "        if (self.isCr):\n",
    "            #print(self.sim[0][0:4])\n",
    "            #print(self.CrMatrix[0][0:4])\n",
    "            self.sim = self.sim * self.CrMatrix\n",
    "            #print(self.sim[0][0:4])\n",
    "        \n",
    "        if(self.isSemantic):\n",
    "            self.sim2 = self.SemanticMatrix #self.sim*self.SemanticMatrix\n",
    "        elif(self.isDemographic):\n",
    "            self.sim2 = self.DemographicMatrix\n",
    "        else:\n",
    "            self.sim2 =  self.sim\n",
    "            \n",
    "        self.means = np.zeros(self.n_x)\n",
    "        for x, ratings in self.xr.items():\n",
    "            self.means[x] = np.mean([r for (_, r) in ratings])   \n",
    "\n",
    "        return self\n",
    "\n",
    "    def estimate(self, u, i):\n",
    "        \n",
    "        if not (self.trainset.knows_user(u) and self.trainset.knows_item(i)):\n",
    "            raise PredictionImpossible(\"User and/or item is unknown.\")\n",
    "\n",
    "        x, y = self.switch(u, i)\n",
    "\n",
    "        est = self.means[x]\n",
    "        est1 = 0\n",
    "        est2 = 0\n",
    "                  \n",
    "        # =========== general CF calculation =========== #\n",
    "        neighbors = [(x_others, self.sim[x, x_others], r) for (x_others, r) in self.yr[y]]\n",
    "        k_neighbors = heapq.nlargest(self.k, neighbors, key=lambda t: t[1])\n",
    "        \n",
    "        # compute weighted average\n",
    "        sum_sim = sum_ratings = actual_k = 0\n",
    "        for (nb, sim, r) in k_neighbors:\n",
    "            if sim > 0:\n",
    "                sum_sim += sim\n",
    "                sum_ratings += sim * (r - self.means[nb])\n",
    "                actual_k += 1 \n",
    "        \n",
    "        if actual_k < self.min_k:\n",
    "            sum_ratings = 0\n",
    "        try:\n",
    "            est1 += sum_ratings / sum_sim\n",
    "        except ZeroDivisionError:\n",
    "            pass  # return mean\n",
    "        \n",
    "        \n",
    "        # =========== semantic or demographic based CF calculation =========== #\n",
    "        if(self.isSemantic or self.isDemographic):\n",
    "            neighbors2 = [(x_others, self.sim2[x, x_others], r) for (x_others, r) in self.yr[y]]\n",
    "            k_neighbors2 = heapq.nlargest(self.k, neighbors2, key=lambda t: t[1])\n",
    "           \n",
    "            # compute weighted average\n",
    "            sum_sim2 = sum_ratings2 = actual_k2 = 0\n",
    "            for (nb2, sim2, r2) in k_neighbors2:\n",
    "                if sim2 > 0:\n",
    "                    sum_sim2 += sim2\n",
    "                    sum_ratings2 += sim2 * (r2 - self.means[nb2])\n",
    "                    actual_k2 += 1\n",
    "                   \n",
    "            if actual_k2 < self.min_k:\n",
    "                sum_ratings2 = 0\n",
    "            try:\n",
    "                est2 += sum_ratings2 / sum_sim2\n",
    "            except ZeroDivisionError:\n",
    "                pass  # return mean\n",
    "        \n",
    "            if est1 == 0 and est2 != 0:\n",
    "                est += est2\n",
    "            elif est2 == 0 and est1 != 0:\n",
    "                est += est1\n",
    "            elif est1 + est2 == 0:\n",
    "                if(est1 > est2):\n",
    "                    est += est1\n",
    "                else:\n",
    "                    est += est2\n",
    "            else:\n",
    "                #est += (2 * est1 * est2)/(est1 + est2)\n",
    "                est += (est1 + est2)/2\n",
    "                #if(self.printLimit<20):\n",
    "                #    print('est1: ', est1,'est2: ', est2, 'est1 + est2: ', est, 'est1 by only:', est + est1, 'est2 by only:', est + est2,)\n",
    "                #    self.printLimit+=1\n",
    "        \n",
    "            details = {\"actual_k\": actual_k, \"actual_k2\": actual_k2}\n",
    "        else:\n",
    "            est += est1\n",
    "            details = {\"actual_k\": actual_k}\n",
    "         \n",
    "        return est, details\n",
    "    \n",
    "class HybridRecommender(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HybridRecommender, self).__init__()\n",
    "        self.fc = nn.Linear(2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1361b6b1-be66-41c5-916e-4749045dbf15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0f6c572e-c059-4257-bfb5-efdda23ab304",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing the cosine similarity matrix...\n",
      "Done computing similarity matrix.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'itemSemanticMatrix' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5908\\2782072651.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;31m# Item-based CF\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[0mitem_based_cf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKNNWithMeans2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m40\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msim_options\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msim_options_forItemBased\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mitem_based_cf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m#user_based_pred = user_based_cf.test(testset)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5908\\3861527873.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, trainset)\u001b[0m\n\u001b[0;32m    265\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 267\u001b[1;33m         \u001b[0mSymmetricAlgo2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrainset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_similarities\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_5908\\3861527873.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, trainset)\u001b[0m\n\u001b[0;32m     42\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     43\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mall_items\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 44\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSemanticMatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mj\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mitemSemanticMatrix\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_raw_iid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrainset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_raw_iid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# Initilizating Demographic matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'itemSemanticMatrix' is not defined"
     ]
    }
   ],
   "source": [
    "#========================================= Main functions starts here =============================================#\n",
    "\n",
    "data = Dataset.load_builtin(\"ml-1m\")\n",
    "\n",
    "trainset, testset = train_test_split(data, test_size=0.05)\n",
    "\n",
    "\n",
    "sim_options_forUserBased = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": True,  # compute  similarities between items\n",
    "    \"isSemantic\": False,\n",
    "    \"isDemographic\": False,\n",
    "    \"isCr\": False\n",
    "}\n",
    "\n",
    "sim_options_forItemBased = {\n",
    "    \"name\": \"cosine\",\n",
    "    \"user_based\": False,  # compute  similarities between items\n",
    "    \"isSemantic\": True,\n",
    "    \"isDemographic\": False,\n",
    "    \"isCr\": False\n",
    "}\n",
    "\n",
    "#algo = KNNBasic2(5, sim_options=sim_options)\n",
    "#algo = KNNBaseline2(10, sim_options=sim_options)\n",
    "#algo = KNNWithMeans2(10, sim_options=sim_options_forUserBased)\n",
    "#cross_validate(algo, data, verbose=True)\n",
    "\n",
    "# User-based CF\n",
    "user_based_cf = KNNWithMeans2(40, sim_options=sim_options_forUserBased)\n",
    "user_based_cf.fit(trainset)\n",
    "\n",
    "\n",
    "# Item-based CF\n",
    "item_based_cf = KNNWithMeans2(40, sim_options=sim_options_forItemBased)\n",
    "item_based_cf.fit(trainset)\n",
    "\n",
    "#user_based_pred = user_based_cf.test(testset)\n",
    "#item_based_pred = item_based_cf.test(testset)\n",
    "\n",
    "#print('User based:', accuracy.rmse(user_based_pred), accuracy.mae(user_based_pred))\n",
    "#print('Item based:', accuracy.rmse(item_based_pred), accuracy.mae(item_based_pred))\n",
    "\n",
    "print(\"Trained using User based and Item based ways\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "cca90b70-a624-4519-932b-ef625e453ebe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.6662694193880094, 2.712232066412871] [3.0]\n",
      "[3.273006743015662, 3.262453833010792] [4.0]\n",
      "[3.8090113976077475, 3.721872541865211] [4.0]\n",
      "[2.953618856282781, 2.5387204215062913] [1.0]\n",
      "[3.521026756202444, 3.562936717453293] [4.0]\n",
      "[3.5327635374503403, 3.744584818917531] [4.0]\n",
      "[3.0537295465103935, 3.0444891737334436] [3.0]\n",
      "[3.2539109617930912, 3.2980736287512156] [4.0]\n",
      "[3.068388254048331, 3.0124678252458126] [3.0]\n",
      "[3.4428726800329255, 3.262218280870847] [3.0]\n"
     ]
    }
   ],
   "source": [
    "model = HybridRecommender()\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "# Prepare input data\n",
    "inputs = []\n",
    "targets = []\n",
    "count = 0\n",
    "for user, item, true_rating in trainset.all_ratings():\n",
    "    uid = trainset.to_raw_uid(user)\n",
    "    iid = trainset.to_raw_iid(item)\n",
    "    user_based_pred = user_based_cf.predict(uid, iid).est\n",
    "    item_based_pred = item_based_cf.predict(uid, iid).est\n",
    "    \n",
    "    #if(count < 10):\n",
    "    #    print(trainset.to_raw_uid(user), trainset.to_raw_iid(item), true_rating, user_based_pred, item_based_pred ) \n",
    "    #    count += 1\n",
    "        \n",
    "    inputs.append([user_based_pred, item_based_pred])\n",
    "    targets.append([float(true_rating)])\n",
    "\n",
    "for d in range(10):\n",
    "    print(inputs[d], targets[d])    \n",
    "\n",
    "inputs = torch.tensor(inputs)\n",
    "targets = torch.tensor(targets)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d997a73b-4fb6-4ac1-8dbe-a23ac6bae31b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([95000, 2])\n",
      "torch.Size([95000, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs = inputs.to(torch.float)\n",
    "targets = targets.to(torch.float)\n",
    "\n",
    "print(inputs.shape)\n",
    "print(targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4570d6ab-61c6-4c69-ac3a-3188fdc2cabd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.8961436838519771\n",
      "RMSE: 0.9466486591402203\n",
      "MAE: 0.7499297465801239\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "epochs = 100\n",
    "for epoch in range(epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(inputs)\n",
    "    loss = criterion(outputs, targets)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "total_mse = 0\n",
    "total_rmse = 0\n",
    "total_mae = 0\n",
    "\n",
    "for uid, iid, true_r in testset:\n",
    "    test_user_based_pred = user_based_cf.predict(uid, iid).est\n",
    "    test_item_based_pred = item_based_cf.predict(uid, iid).est\n",
    "    \n",
    "    test_input_data = torch.tensor([[test_user_based_pred, test_item_based_pred]])\n",
    "    test_input_data = test_input_data.to(torch.float)\n",
    "    test_predicted_rating = model(test_input_data).item()\n",
    "    total_mse += (test_predicted_rating - true_r) ** 2\n",
    "    \n",
    "    diff = test_predicted_rating - true_r\n",
    "    total_rmse += diff ** 2\n",
    "    total_mae += abs(diff)\n",
    "\n",
    "mse = total_mse / len(testset)\n",
    "print(f\"Mean Squared Error: {mse}\")\n",
    "\n",
    "n_testset = len(testset)\n",
    "rmse = (total_rmse / n_testset) ** 0.5\n",
    "mae = total_mae / n_testset\n",
    "\n",
    "print(\"RMSE:\", rmse)\n",
    "print(\"MAE:\", mae)\n",
    "\n",
    "#print('User based:', accuracy.rmse(user_based_pred), accuracy.mae(user_based_pred))\n",
    "#print('Item based:', accuracy.rmse(item_based_pred), accuracy.mae(item_based_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6894697-63f9-407c-b76e-92a7d0d7403e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
